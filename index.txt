
.. _index:

======================
Two!Ears documentation
======================

**Everything you need to know about the Two!Ears Auditory Model.**

.. rubric:: Getting help

Having trouble? We'd like to help!

* Looking for specific information? Try
  the :doc:`detailed table of contents <contents>`.

* Search for information or post a question in the
  |TwoearsMailingList|.

* Report bugs or ask for new features in our `issue tracker`_.

.. _archives: http://groups.google.com/group/twoears/
.. _issue tracker: https://github.com/TWOEARS/TwoEars/issues

.. rubric:: First steps

New to Two!Ears or auditory modelling? This is the place to start!

* **Introduction:**
  :ref:`sec-first-steps` |
  :ref:`sec-install` |
  :ref:`sec-modules`

* **Tutorials:**
  :ref:`Part 1: Setting up an acoustic scene <sec-tutorial-acoustic-scene>` |
  :ref:`Part 2: Setting up an auditory model <sec-tutorial-auditory-model>` |
  :ref:`Part 3: Working with the database <sec-tutorial-database>` |
  :ref:`Part 4: Use a robotic plattform <sec-tutorial-robot>`

.. rubric:: Binaural simulator

The Two!Ears Binaural Simulator enables you to create dynamic binaural ear
signals, that can be used by later parts of the model to actively explore a
scene.

* :ref:`Overview <sec-binsim>`
* :ref:`sec-binsim-usage`
* :ref:`sec-binsim-examples`
* :ref:`sec-binsim-install`

.. rubric:: Robotic platform

If you have a robotic platform ready to record the binaural signals, no
simulation of them will be needed. Here you find the software you need to
connect Matlab with the robotic world:

* :ref:`sec-robot-getting-started`
* :ref:`sec-robot-audio-streaming`

.. rubric:: Auditory front-end

The Two!Ears Auditory Front-End extracts all kinds of auditory cues from the ear
signals like the loudness or interaural differences. For detailed description
see:

* :ref:`Introduction <sec-afe>`
* :ref:`sec-afe-overview`
* :ref:`sec-afe-technical-description`
* :ref:`sec-afe-processors`
* :ref:`sec-afe-add-processor`

.. rubric:: Blackboard system

The Two!Ears Blackboard System is the *brain* of the Two!Ears Auditory Model as
it provides a way to interpret the auditory cues and extract meaning from them.
Learn how this happens below:

* :ref:`sec-blackboard-introduction`
* :ref:`sec-blackboard-usage`
* :ref:`sec-blackboard-architecture`
* :ref:`sec-knowledge-sources`
* :ref:`sec-blackboard-add-knowledge-source`
* :ref:`sec-blackboard-training`

.. rubric:: Database

The Two!Ears Binaural Simulator and the Two!Ears Blackboard System uses a lot of
different data to perform their tasks. This is achieved by a large collection of
data:

* :ref:`Overview <sec-database>`
* :ref:`sec-database-usage`
* :ref:`sec-listening-tests`
* :ref:`sec-impulse-responses`
* :ref:`sec-sound-databases`
* :ref:`sec-database-stimuli`

.. rubric:: Examples

A key concept of the Two!Ears Auditory Model is reproducible research. Here, you
will find scripts show casing basic usage examples of the model or redoing
figures from our publications:

* :ref:`sec-examples-localisation`
* :ref:`sec-examples-localisation-details`
* :ref:`sec-examples-train-localisation`
* :ref:`sec-examples-train-identification`
* :ref:`sec-examples-identification`
* :ref:`sec-examples-tutorial-bass`

.. rubric:: Development

If you are part of the Two!Ears development team or would like to become part of
it, read on:

* :ref:`How to get involved <sec-dev>`
* :ref:`sec-coding-style-guide`
* :ref:`Writing this documentation <sec-write-documentation>`
* :ref:`sec-git`

.. rubric:: License

If not otherwise stated inside single files the |TwoEarsModel| is licensed
under `GNU General Public License, version 3`_, and its parts available in the
``RoboticPlatform`` and the ``Tools`` folder are licensed under `The BSD
2-Clause License`_.

.. _GNU General Public License, version 3: http://www.gnu.org/licenses/gpl-3.0.html
.. _The BSD 2-Clause License: http://twoears.github.io/bsd-2-clause-license

.. rubric:: Acknowledgement

The Two!Ears team are in alphabetical order:

Sylvain Argentieri (UPMC),
Jens Blauert (RUB),
Jonas Braasch (Rensselaer),
Guy Brown (USFD),
Benjamin Cohen-L'hyver (UPMC),
Patrick Danès (LAAS),
Torsten Dau (DTU),
Rémi Decorsière (DTU),
Thomas Forgue (LAAS),
Bruno Gas (UPMC),
Youssef Kashef (TUB),
Chungeun Kim (TU/e),
Armin Kohlrausch (TU/e),
Dorothea Kolossa (RUB),
Ning Ma (USFD),
Tobias May (DTU),
Johannes Mohr (TUB),
Antonyo Musabini (UPMC),
Klaus Obermayer (TUB),
Ariel Podlubne (LAAS),
Alexander Raake (TUIl),
Christopher Schymura (RUB),
Sascha Spors (URO),
Jalil Taghia (TUB),
Ivo Trowitzsch (TUB),
Thomas Walther (RUB),
Hagen Wierstorf (TUIl),
Fiete Winter (URO).

This project has received funding from the European Union's Seventh Framework
Programme for research, technological development and demonstration under grant
agreement no 618075.

.. image:: img/eu-flag.png

.. image:: img/tree.jpg
    :target: http://cordis.europa.eu/fet-proactive/

.. vim: filetype=rst spell:
